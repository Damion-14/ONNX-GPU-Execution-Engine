#include "model_parser.hpp"
#include "../utils/logger.hpp"
#include "onnx.pb.h"
#include <fstream>
#include <stdexcept>

namespace onnx_runner {

std::shared_ptr<Graph> ModelParser::parse(const std::string& model_path) {
    LOG_INFO("Parsing ONNX model: ", model_path);

    // Read the file
    std::ifstream input(model_path, std::ios::binary);
    if (!input) {
        throw std::runtime_error("Failed to open model file: " + model_path);
    }

    // Parse the protobuf
    onnx::ModelProto model;
    if (!model.ParseFromIstream(&input)) {
        throw std::runtime_error("Failed to parse ONNX model");
    }

    LOG_INFO("Model IR version: ", model.ir_version());
    LOG_INFO("Model producer: ", model.producer_name(), " ", model.producer_version());

    // Get the graph from the model
    const onnx::GraphProto& onnx_graph = model.graph();
    LOG_INFO("Graph name: ", onnx_graph.name());

    auto graph = std::make_shared<Graph>();

    // Parse initializers (constant tensors like weights and biases)
    LOG_INFO("Parsing ", onnx_graph.initializer_size(), " initializers...");
    for (int i = 0; i < onnx_graph.initializer_size(); ++i) {
        const onnx::TensorProto& tensor_proto = onnx_graph.initializer(i);
        std::string name = tensor_proto.name();

        auto tensor = parseTensorProto(&tensor_proto);
        graph->addInitializer(name, tensor);

        LOG_DEBUG("  Initializer: ", name, " ", tensor->shapeStr());
    }

    // Parse graph inputs
    LOG_INFO("Parsing ", onnx_graph.input_size(), " inputs...");
    for (int i = 0; i < onnx_graph.input_size(); ++i) {
        const onnx::ValueInfoProto& input = onnx_graph.input(i);
        std::string name = input.name();

        // Skip if it's an initializer (initializers are also listed as inputs in ONNX)
        if (!graph->isInitializer(name)) {
            graph->addInput(name);
            LOG_DEBUG("  Input: ", name);
        }
    }

    // Parse graph outputs
    LOG_INFO("Parsing ", onnx_graph.output_size(), " outputs...");
    for (int i = 0; i < onnx_graph.output_size(); ++i) {
        const onnx::ValueInfoProto& output = onnx_graph.output(i);
        graph->addOutput(output.name());
        LOG_DEBUG("  Output: ", output.name());
    }

    // Parse nodes
    LOG_INFO("Parsing ", onnx_graph.node_size(), " nodes...");
    for (int i = 0; i < onnx_graph.node_size(); ++i) {
        const onnx::NodeProto& onnx_node = onnx_graph.node(i);

        // Get node name (use output name if node name is empty)
        std::string node_name = onnx_node.name();
        if (node_name.empty() && onnx_node.output_size() > 0) {
            node_name = onnx_node.output(0);
        }

        // Convert op_type
        OpType op_type = stringToOpType(onnx_node.op_type());
        if (op_type == OpType::UNKNOWN) {
            LOG_WARN("  Unknown op type: ", onnx_node.op_type(), " in node ", node_name);
        }

        auto node = std::make_shared<Node>(node_name, op_type);

        // Add inputs
        for (int j = 0; j < onnx_node.input_size(); ++j) {
            node->addInput(onnx_node.input(j));
        }

        // Add outputs
        for (int j = 0; j < onnx_node.output_size(); ++j) {
            node->addOutput(onnx_node.output(j));
        }

        // Parse attributes
        for (int j = 0; j < onnx_node.attribute_size(); ++j) {
            const onnx::AttributeProto& attr = onnx_node.attribute(j);
            std::string attr_name = attr.name();

            switch (attr.type()) {
                case onnx::AttributeProto::INT:
                    node->setAttribute(attr_name, AttributeValue::fromInt(attr.i()));
                    break;
                case onnx::AttributeProto::FLOAT:
                    node->setAttribute(attr_name, AttributeValue::fromFloat(attr.f()));
                    break;
                case onnx::AttributeProto::STRING:
                    node->setAttribute(attr_name, AttributeValue::fromString(attr.s()));
                    break;
                case onnx::AttributeProto::INTS: {
                    std::vector<int64_t> values;
                    for (int k = 0; k < attr.ints_size(); ++k) {
                        values.push_back(attr.ints(k));
                    }
                    node->setAttribute(attr_name, AttributeValue::fromInts(values));
                    break;
                }
                default:
                    LOG_WARN("  Unsupported attribute type for ", attr_name);
                    break;
            }
        }

        graph->addNode(node);
        LOG_DEBUG("  Node: ", opTypeToString(op_type), " (", node_name, ")");
    }

    LOG_INFO("Model parsed successfully!");
    return graph;
}

DataType ModelParser::onnxDataTypeToDataType(int onnx_type) {
    // ONNX data types from onnx.proto
    switch (onnx_type) {
        case 1:  // FLOAT
            return DataType::FLOAT32;
        case 6:  // INT32
            return DataType::INT32;
        case 7:  // INT64
            return DataType::INT64;
        case 10: // FLOAT16
            return DataType::FLOAT16;
        case 2:  // UINT8
            return DataType::UINT8;
        default:
            LOG_WARN("Unknown ONNX data type: ", onnx_type, ", defaulting to FLOAT32");
            return DataType::FLOAT32;
    }
}

std::shared_ptr<Tensor> ModelParser::parseTensorProto(const void* proto_ptr) {
    const onnx::TensorProto* tensor_proto = static_cast<const onnx::TensorProto*>(proto_ptr);

    // Extract shape
    std::vector<int64_t> shape;
    for (int i = 0; i < tensor_proto->dims_size(); ++i) {
        shape.push_back(tensor_proto->dims(i));
    }

    // Get data type
    DataType dtype = onnxDataTypeToDataType(tensor_proto->data_type());

    // Create tensor
    auto tensor = std::make_shared<Tensor>(shape, dtype);

    // Extract data
    // ONNX can store data in multiple formats - we handle the most common ones
    if (dtype == DataType::FLOAT32) {
        float* data = tensor->data<float>();
        size_t size = tensor->size();

        if (tensor_proto->float_data_size() > 0) {
            // Data is stored in float_data field
            for (size_t i = 0; i < size && i < (size_t)tensor_proto->float_data_size(); ++i) {
                data[i] = tensor_proto->float_data(i);
            }
        } else if (tensor_proto->has_raw_data()) {
            // Data is stored in raw_data field
            const std::string& raw_data = tensor_proto->raw_data();
            size_t bytes = std::min(size * sizeof(float), raw_data.size());
            std::memcpy(data, raw_data.data(), bytes);
        }
    } else if (dtype == DataType::INT64) {
        int64_t* data = tensor->data<int64_t>();
        size_t size = tensor->size();

        if (tensor_proto->int64_data_size() > 0) {
            for (size_t i = 0; i < size && i < (size_t)tensor_proto->int64_data_size(); ++i) {
                data[i] = tensor_proto->int64_data(i);
            }
        } else if (tensor_proto->has_raw_data()) {
            const std::string& raw_data = tensor_proto->raw_data();
            size_t bytes = std::min(size * sizeof(int64_t), raw_data.size());
            std::memcpy(data, raw_data.data(), bytes);
        }
    }
    // Add more data types as needed

    return tensor;
}

} // namespace onnx_runner
